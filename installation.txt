# KIND + Monitoring + N4K Reports-Server Installation Playbook

# 0) Clean up (optional)
kind delete cluster --name kyverno-reports-test

# 1) Create 3-node KIND cluster
kind create cluster --config kind-config.yaml --wait 600s
kubectl get nodes -o wide

# 2) Install Prometheus + Grafana (kube-prometheus-stack)
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update
helm upgrade --install monitoring prometheus-community/kube-prometheus-stack -n monitoring --create-namespace \
  --set grafana.enabled=true \
  --set grafana.service.type=NodePort \
  --set grafana.service.nodePort=30001 \
  --set prometheus.service.type=NodePort \
  --set prometheus.service.nodePort=30000

# Check components
kubectl -n monitoring get pods -o wide

# Grafana admin password
kubectl -n monitoring get secret monitoring-grafana -o jsonpath='{.data.admin-password}' | base64 -d ; echo

# 3) Baseline etcd size (before N4K + Reports Server)
ETCD_POD=$(kubectl get pods -n kube-system -l component=etcd -o jsonpath='{.items[0].metadata.name}')
kubectl -n kube-system exec $ETCD_POD -- sh -c "ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/peer.crt --key=/etc/kubernetes/pki/etcd/peer.key endpoint status --write-out=table"

# 4) Install Reports Server (must be installed BEFORE N4K 1.14)
# Chart version pinned: reports-server-0.2.3 (app v0.2.2)
helm repo add rs https://nirmata.github.io/reports-server/
helm repo update
helm upgrade --install reports-server rs/reports-server \
  --namespace kyverno --create-namespace \
  --version 0.2.3

# 5) Install N4K 1.14 (Kyverno n4k)
# Chart version pinned: kyverno-3.4.7 (Kyverno v1.14.3-n4k.nirmata.4)
helm repo add nirmata https://nirmata.github.io/kyverno-charts/
helm repo update
helm upgrade --install kyverno nirmata/kyverno \
  --namespace kyverno --create-namespace \
  --version 3.4.7

# 6) Apply ServiceMonitors so Prometheus scrapes Kyverno and Reports-Server
kubectl apply -f /Users/anujramola/Devtest/Project_Workflow/reports-server-servicemonitor.yaml
kubectl apply -f /Users/anujramola/Devtest/Project_Workflow/kyverno-servicemonitor.yaml
kubectl apply -f /Users/anujramola/Devtest/Project_Workflow/reports-server-etcd-servicemonitor.yaml
kubectl -n monitoring get servicemonitors

# 7) Post-install etcd size (after N4K + Reports Server)
kubectl -n kube-system exec $ETCD_POD -- sh -c "ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/peer.crt --key=/etc/kubernetes/pki/etcd/peer.key endpoint status --write-out=table"

# 8) Apply baseline policies (from GitHub via kustomize)
# Source repo: https://github.com/nirmata/kyverno-policies/tree/main/pod-security/baseline
# Clone once, then apply using kubectl kustomize → apply
test -d kyverno-policies || git clone --depth 1 https://github.com/nirmata/kyverno-policies.git
kubectl kustomize kyverno-policies/pod-security/baseline | kubectl apply -f -

# Alternative (no local clone, requires kubectl with remote kustomize support):
# kubectl apply -k https://github.com/nirmata/kyverno-policies//pod-security/baseline?ref=main

# 9) Verify reports
kubectl get polr -A

# 10) Monitoring views
# - Kyverno: Import the Grafana dashboard JSON in this repo (`kyverno-dashboard.json`).
#   Grafana → Dashboards → Import → Upload JSON file → select kyverno-dashboard.json.
# - etcd (Kubernetes control plane): use the etcdctl endpoint status commands above (steps 3 and 7).
# - etcd metrics: under investigation for this setup.

# 11) Teardown (optional)
kind delete cluster --name kyverno-reports-test

